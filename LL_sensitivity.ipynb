{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global packages\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "#SB3\n",
    "from stable_baselines3 import DQN, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common import logger\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Video\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "from IPython.display import HTML, display\n",
    "import os\n",
    "\n",
    "# Analysis\n",
    "import pandas as pd\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for local storage\n",
    "base_path = \"/Users/maartendoekhie/Desktop/lunar_lander\"\n",
    "\n",
    "# Folder structure\n",
    "folders = {\n",
    "    \"models\": os.path.join(base_path, \"models\"),\n",
    "    \"videos\": os.path.join(base_path, \"videos\"),\n",
    "    \"results\": os.path.join(base_path, \"results\"),\n",
    "    \"logs\": os.path.join(base_path, \"logs\"),\n",
    "    \"plots\": os.path.join(base_path, \"plots\"),\n",
    "    \"logs_dqn\": os.path.join(base_path, \"logs\", \"dqn_tensorboard\"),\n",
    "    \"logs_a2c\": os.path.join(base_path, \"logs\", \"a2c_tensorboard\"),\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in folders.values():\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create your custom environment\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    continuous=False,\n",
    "    gravity=-10.0,\n",
    "    enable_wind=False,\n",
    "    wind_power=15.0,\n",
    "    turbulence_power=1.5,\n",
    ")\n",
    "\n",
    "# Wrap in DummyVecEnv and then VecMonitor\n",
    "env_vec = DummyVecEnv([lambda: env])\n",
    "env_vec = VecMonitor(env_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action space: What can your agent do?\n",
    "\n",
    "# 0: do nothing\n",
    "# 1: fire left orientation engine\n",
    "# 2: fire main engine\n",
    "# 3: fire right orientation engine\n",
    "\n",
    "action = env.action_space # not needed for now\n",
    "action_size = 4\n",
    "\n",
    "# Observation space: What can your agent see?\n",
    "\n",
    "# The state is an 8-dimensional vector:\n",
    "\n",
    "# the coordinates of the lander in x & y,\n",
    "# its linear velocities in x & y, its angle,\n",
    "# its angular velocity,\n",
    "# and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "state = env.observation_space.shape # not needed for now\n",
    "state_size = 8\n",
    "\n",
    "### Vectorized version of the environment ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPER PARAMETERS ###\n",
    "\n",
    "if debug == True:\n",
    "  # Replay Memory\n",
    "  replay_buffer_size = 10000 \n",
    "  minibatch = 32\n",
    "\n",
    "  # learning\n",
    "  learning_rate = 5e-4\n",
    "  gamma = 0.99 \n",
    "  interpolation_parameter = 1e-3\n",
    "\n",
    "  # training\n",
    "  number_episodes = 5 \n",
    "  max_time_steps = 200\n",
    "\n",
    "  # epsilon gready policy\n",
    "  epsilon_starting_value = 1.0\n",
    "  epsilon_ending_value = 0.01\n",
    "  epsilon_decay_value = 0.995\n",
    "\n",
    "  # evaluation\n",
    "  n_eval_episodes = 3\n",
    "\n",
    "  # SB3\n",
    "  total_timesteps = 10000 \n",
    "  batch_size = minibatch\n",
    "  buffer_size = replay_buffer_size\n",
    "\n",
    "else: \n",
    "  # Replay Memory\n",
    "  replay_buffer_size = 100000\n",
    "  minibatch = 150\n",
    "\n",
    "  # learning\n",
    "  learning_rate = 5e-4\n",
    "  gamma = 0.99 \n",
    "  interpolation_parameter = 1e-3 \n",
    "\n",
    "  # training\n",
    "  number_episodes = 5000\n",
    "  max_time_steps = 1000\n",
    "\n",
    "  # epsilon gready policy\n",
    "  epsilon_starting_value = 1.0\n",
    "  epsilon_ending_value = 0.01\n",
    "  epsilon_decay_value = 0.995\n",
    "\n",
    "  # evaluation\n",
    "  n_eval_episodes = 10\n",
    "\n",
    "  # SB3\n",
    "  total_timesteps = 10000000\n",
    "  batch_size = minibatch\n",
    "  buffer_size = replay_buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "### CUSTOM DQN AGENT SETUP ###\n",
    "##############################\n",
    "\n",
    "# Consisting of the DQN model, Agent (with implemented learning and training), Replay Memory\n",
    "\n",
    "### DQN MODEL SETUP ###\n",
    "\n",
    "# We wan't to start with a simple 2 layer fc NN. Starting from state size to action size\n",
    "# Shallow problems 1 to 2 hidden layers and complex tasks 3 to 4 hidden layers\n",
    "# Underfitting: add more layers/neurons, overfitting: add droppout, regularization or reduce size\n",
    "# Slow learning: reduce depth or learning rate\n",
    "\n",
    "class DQN_custom(nn.Module):\n",
    "  def __init__(self, state_size, action_size, seed = 4):\n",
    "    super(DQN_custom, self).__init__()\n",
    "    self.seed = torch.manual_seed(seed)\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(state_size, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, action_size)\n",
    "    )\n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "\n",
    "# the memory can store an event: s, a,r, s' and done (with a maximum capacity: i.e. hyper parameter)\n",
    "# the memory can give a sample: s, a, r, s' and done in a minibatch. i.e. matrices: S, A, R, R_, D\n",
    "\n",
    "# 1. s:      the current state (before the action was taken)\n",
    "#            This is the input to the Q-network and represents the agent’s current observation of the environment.\n",
    "\n",
    "# 2. a:      the action taken by the agent in state s\n",
    "#            This action is chosen according to an exploration policy (e.g., ε-greedy) and used to interact with the environment.\n",
    "\n",
    "# 3. r:      the reward received after taking action a in state s\n",
    "#            This tells the agent how good or bad the outcome of that action was.\n",
    "\n",
    "# 4. s':     the next state observed after taking action a\n",
    "#            This is the new observation received from the environment after applying the action.\n",
    "\n",
    "# 5. done:   a boolean flag indicating if the episode ended after this step\n",
    "#            If done is True, then s' is the terminal state and no further actions should be taken.\n",
    "\n",
    "\n",
    "### DQN MEMORY ###\n",
    "\n",
    "class Memory(object):\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.memory = deque(maxlen=capacity)\n",
    "\n",
    "  def store(self, event):\n",
    "    self.memory.append(event)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    if len(self.memory) < batch_size:# Not enough samples yet: tell the agent to skip learning\n",
    "\n",
    "      return None\n",
    "\n",
    "    else:\n",
    "      minibatch = random.sample(self.memory, batch_size)\n",
    "      state_list      = []\n",
    "      action_list     = []\n",
    "      reward_list     = []\n",
    "      next_state_list = []\n",
    "      done_list       = []\n",
    "\n",
    "      for experiences in minibatch: # for every experience in the minibatch, we have a sars', done.\n",
    "        if experiences is not None:\n",
    "          state, action, reward, next_state, done = experiences\n",
    "          state_list.append(state)\n",
    "          action_list.append(action)\n",
    "          reward_list.append(reward)\n",
    "          next_state_list.append(next_state)\n",
    "          done_list.append(done)\n",
    "\n",
    "          # 1. states:      shape = (batch_size, 8)\n",
    "          #    Each row is a full state vector from LunarLander (8 floats: x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact)\n",
    "\n",
    "          # 2. actions:     shape = (batch_size, 1)\n",
    "          #    Each row contains the action taken (integer 0–3), one per experience\n",
    "\n",
    "          # 3. rewards:     shape = (batch_size, 1)\n",
    "          #    Each row contains the scalar reward received after taking the action\n",
    "\n",
    "          # 4. next_states: shape = (batch_size, 8)\n",
    "          #    Each row is the resulting state after the action was taken (same format as states)\n",
    "\n",
    "          # 5. dones:       shape = (batch_size, 1)\n",
    "          #    Each row is 1.0 if the episode ended after this transition, else 0.0\n",
    "\n",
    "          # We wan't to create matrices of these events. with size explained above\n",
    "\n",
    "      S = torch.from_numpy(np.vstack(state_list)).float()\n",
    "      A = torch.from_numpy(np.vstack(action_list)).long()\n",
    "      R = torch.from_numpy(np.vstack(reward_list)).float()\n",
    "      R_ = torch.from_numpy(np.vstack(next_state_list)).float()\n",
    "      D = torch.from_numpy(np.vstack(done_list).astype(np.uint8)).float()\n",
    "\n",
    "      return S, A, R, R_, D\n",
    "\n",
    "### DQN AGENT ###\n",
    "\n",
    "class Agent():\n",
    "\n",
    "  def __init__(self, state_size, action_size):\n",
    "      self.action_size = action_size\n",
    "      self.state_size = state_size\n",
    "\n",
    "      self.local_qnetwork = DQN_custom(state_size, action_size)\n",
    "      self.target_qnetwork = DQN_custom(state_size, action_size)\n",
    "\n",
    "      self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
    "      self.memory = Memory(replay_buffer_size)\n",
    "      self.t_step = 0\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "      self.memory.store((state, action, reward, next_state, done))\n",
    "      self.t_step = (self.t_step + 1) % 4\n",
    "      if self.t_step == 0:\n",
    "          experiences = self.memory.sample(minibatch)\n",
    "          if experiences is not None:\n",
    "              self.learn(experiences, gamma)\n",
    "\n",
    "  def get_action(self, state, epsilon):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0) # pytorch expects a tensor batch\n",
    "    self.local_qnetwork.eval() # avoid unwanted updates\n",
    "\n",
    "    with torch.no_grad(): # passing the state through the network without calculating gradients.\n",
    "        action_values = self.local_qnetwork(state)  # [Q(state, action_0), Q(state, action_1), ...]\n",
    "    self.local_qnetwork.train() # putting network back into the training network\n",
    "\n",
    "    if random.random() > epsilon: # exploration, exploitation tradeoff, epsilon-greedy exploration?\n",
    "        return np.argmax(action_values.cpu().data.numpy()) # get the action with the highest q-value, exploitation part\n",
    "    else:\n",
    "        return random.choice(np.arange(self.action_size)) # else the agent will pick a random move. This happens when the agent, decides to explore rather than exploit.\n",
    "\n",
    "  def learn(self, experiences, gamma): # training an AI agent in reinforcement learning, bellman adaptation for deep q learning is used (maybe here use something from the lecture slides)\n",
    "\n",
    "      states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "      next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1) # Check pictures\n",
    "\n",
    "      q_targets = self.bellman_optimality(rewards, next_q_targets, dones, gamma) # Bellman equation on the target network,\n",
    "\n",
    "      q_expected = self.local_qnetwork(states).gather(1, actions) # expected calculaton on the local network\n",
    "\n",
    "      loss = nn.MSELoss() # (Q_target - Q_expected)^2\n",
    "      loss = loss(q_expected, q_targets)\n",
    "\n",
    "      self.optimizer.zero_grad() # classic!\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      self.td_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter) # See temporal difference updating from slides > implement in report.\n",
    "\n",
    "  def bellman_optimality(self, rewards, next_q_values, dones, gamma):\n",
    "    return rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "  def td_update(self, local_qnetwork, target_qnetwork, interpolation_parameter):\n",
    "    local_parameters = list(local_qnetwork.parameters())\n",
    "    target_parameters = list(target_qnetwork.parameters())\n",
    "\n",
    "    # Loop through each parameter index\n",
    "    for i in range(len(local_parameters)):\n",
    "        local_param = local_parameters[i]\n",
    "        target_param = target_parameters[i]\n",
    "\n",
    "        # Perform the soft update (temporal difference update)\n",
    "        updated_param = interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data\n",
    "        target_param.data.copy_(updated_param)\n",
    "\n",
    "  def train(self, env, number_episodes, max_time_steps, epsilon_starting_value, epsilon_ending_value, epsilon_decay_value):\n",
    "      episode_scores = []\n",
    "      moving_avg_scores = []\n",
    "      epsilon_values = []\n",
    "      episode_lengths = []\n",
    "      timestep_counts = []\n",
    "      cumulative_timesteps = 0\n",
    "\n",
    "      epsilon = epsilon_starting_value\n",
    "\n",
    "      for i in range(number_episodes):  # episodes\n",
    "          if i > 2000:\n",
    "              print(\"Training aborted: episode limit exceeded.\")\n",
    "              return (\n",
    "                  [np.nan], [np.nan], [np.nan], [np.nan], [np.nan]\n",
    "              )\n",
    "\n",
    "          state, info = env.reset()\n",
    "          score = 0\n",
    "\n",
    "          for j in range(max_time_steps):\n",
    "              action = self.get_action(state, epsilon)\n",
    "              next_state, reward, done, _, _ = env.step(action)\n",
    "              self.step(state, action, reward, next_state, done)\n",
    "              state = next_state\n",
    "              score += reward\n",
    "              if done:\n",
    "                  break\n",
    "\n",
    "          episode_lengths.append(j + 1)\n",
    "          episode_scores.append(score)\n",
    "          cumulative_timesteps += (j + 1)\n",
    "          timestep_counts.append(cumulative_timesteps)\n",
    "\n",
    "          if len(episode_scores) >= 100:\n",
    "              avg_score = np.mean(episode_scores[-100:])\n",
    "          else:\n",
    "              avg_score = np.mean(episode_scores)\n",
    "\n",
    "          moving_avg_scores.append(avg_score)\n",
    "\n",
    "          epsilon = max(epsilon_ending_value, epsilon * epsilon_decay_value)\n",
    "          epsilon_values.append(epsilon)\n",
    "\n",
    "          if i % 100 == 0:\n",
    "              print(f\"Episode {i} | Average Score (last 100 episodes): {avg_score:.2f}\")\n",
    "          if avg_score >= 200.0:\n",
    "              break\n",
    "\n",
    "      return moving_avg_scores, episode_scores, epsilon_values, episode_lengths, timestep_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, n_eval_episodes=10):\n",
    "    evaluation_rewards = []\n",
    "    evaluation_lengths = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Use greedy policy: epsilon = 0\n",
    "            action = agent.get_action(state, epsilon=0.0)\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        evaluation_rewards.append(total_reward)\n",
    "        evaluation_lengths.append(steps)\n",
    "\n",
    "    mean_reward = np.mean(evaluation_rewards)\n",
    "    std_reward = np.std(evaluation_rewards)\n",
    "\n",
    "    return evaluation_rewards, evaluation_lengths, mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping epsilon_0.8__gamma_0.95__batch_125 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.95__batch_150 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.95__batch_175 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.97__batch_125 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.97__batch_150 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.97__batch_175 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.99__batch_125 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.99__batch_150 (already done)\n",
      "Skipping epsilon_0.8__gamma_0.99__batch_175 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.95__batch_125 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.95__batch_150 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.95__batch_175 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.97__batch_125 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.97__batch_150 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.97__batch_175 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.99__batch_125 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.99__batch_150 (already done)\n",
      "Skipping epsilon_0.9__gamma_0.99__batch_175 (already done)\n",
      "Skipping epsilon_1.0__gamma_0.95__batch_125 (already done)\n",
      "Skipping epsilon_1.0__gamma_0.95__batch_150 (already done)\n",
      "Skipping epsilon_1.0__gamma_0.95__batch_175 (already done)\n",
      "Skipping epsilon_1.0__gamma_0.97__batch_125 (already done)\n",
      "Skipping epsilon_1.0__gamma_0.97__batch_150 (already done)\n",
      "\n",
      "--- Evaluating for ε_start=1.0, γ=0.97, minibatch=175 ---\n",
      "\n",
      "--- Evaluating for ε_start=1.0, γ=0.99, minibatch=125 ---\n",
      "\n",
      "--- Evaluating for ε_start=1.0, γ=0.99, minibatch=150 ---\n",
      "\n",
      "--- Evaluating for ε_start=1.0, γ=0.99, minibatch=175 ---\n",
      "\n",
      "Top configurations by mean reward:\n",
      "    epsilon_start  gamma  batch_size  mean_reward  std_reward  mean_length\n",
      "15            0.9   0.99         125   251.784381   20.268540        286.8\n",
      "18            1.0   0.95         125   251.235186   24.009504        494.0\n",
      "9             0.9   0.95         125   244.755993   47.050778        345.4\n",
      "24            1.0   0.99         125   240.752668   21.348376        384.7\n",
      "20            1.0   0.95         175   237.265629   48.864854        380.9\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# === Define hyperparameter ranges ===\n",
    "epsilon_start_values = [0.8, 0.9, 1.0]\n",
    "gamma_values = [0.95, 0.97, 0.99]\n",
    "batch_sizes = [125, 150, 175]\n",
    "\n",
    "# === Fixed parameters ===\n",
    "epsilon_ending_value = 0.01\n",
    "epsilon_decay_value = 0.995\n",
    "interpolation_parameter = 1e-3\n",
    "replay_buffer_size = 100000\n",
    "learning_rate = 5e-4\n",
    "n_eval_episodes = 10\n",
    "\n",
    "# === Save directory ===\n",
    "base_dir = \"/Users/maartendoekhie/Desktop/lunar_lander/results/sensitivity\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# === Load trained agent once ===\n",
    "agent = Agent(state_size, action_size)\n",
    "agent.optimizer = optim.Adam(agent.local_qnetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load trained Q-network weights\n",
    "local_q_path = \"/Users/maartendoekhie/Desktop/lunar_lander/models/local_qnetwork_DQN.pt\"\n",
    "target_q_path = \"/Users/maartendoekhie/Desktop/lunar_lander/models/local_qnetwork_DQN.pt\"\n",
    "\n",
    "agent.local_qnetwork.load_state_dict(torch.load(local_q_path, map_location=\"cpu\"))\n",
    "agent.target_qnetwork.load_state_dict(torch.load(target_q_path, map_location=\"cpu\"))\n",
    "\n",
    "agent.local_qnetwork.eval()\n",
    "agent.target_qnetwork.eval()\n",
    "\n",
    "# === Results summary ===\n",
    "results = []\n",
    "\n",
    "# === Loop over all parameter combinations ===\n",
    "for epsilon_starting_value, gamma, minibatch in product(epsilon_start_values, gamma_values, batch_sizes):\n",
    "    folder_name = f\"epsilon_{epsilon_starting_value}__gamma_{gamma}__batch_{minibatch}\"\n",
    "    save_folder = os.path.join(base_dir, folder_name)\n",
    "    rewards_file = os.path.join(save_folder, \"eval_rewards.npy\")\n",
    "    lengths_file = os.path.join(save_folder, \"eval_lengths.npy\")\n",
    "\n",
    "    # Skip if this configuration was already completed\n",
    "    if os.path.exists(rewards_file) and os.path.exists(lengths_file):\n",
    "        print(f\"Skipping {folder_name} (already done)\")\n",
    "        # Optionally load existing results to append to summary\n",
    "        try:\n",
    "            eval_rewards = np.load(rewards_file)\n",
    "            eval_lengths = np.load(lengths_file)\n",
    "            results.append({\n",
    "                'epsilon_start': epsilon_starting_value,\n",
    "                'gamma': gamma,\n",
    "                'batch_size': minibatch,\n",
    "                'mean_reward': np.mean(eval_rewards),\n",
    "                'std_reward': np.std(eval_rewards),\n",
    "                'mean_length': np.mean(eval_lengths)\n",
    "            })\n",
    "        except Exception:\n",
    "            print(f\"⚠️ Could not reload saved results for {folder_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Evaluating for ε_start={epsilon_starting_value}, γ={gamma}, minibatch={minibatch} ---\")\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Update agent evaluation parameters\n",
    "    agent.batch_size = minibatch\n",
    "    agent.gamma = gamma\n",
    "\n",
    "    # Evaluate agent\n",
    "    try:\n",
    "        eval_rewards, eval_lengths, mean_reward, std_reward = evaluate_agent(\n",
    "            agent, env, n_eval_episodes=n_eval_episodes\n",
    "        )\n",
    "        mean_length = np.mean(eval_lengths)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Evaluation failed: {e}\")\n",
    "        eval_rewards = np.full(n_eval_episodes, np.nan)\n",
    "        eval_lengths = np.full(n_eval_episodes, np.nan)\n",
    "        mean_reward = np.nan\n",
    "        std_reward = np.nan\n",
    "        mean_length = np.nan\n",
    "\n",
    "    # Save numpy arrays\n",
    "    np.save(rewards_file, eval_rewards)\n",
    "    np.save(lengths_file, eval_lengths)\n",
    "\n",
    "    # Append to results summary\n",
    "    results.append({\n",
    "        'epsilon_start': epsilon_starting_value,\n",
    "        'gamma': gamma,\n",
    "        'batch_size': minibatch,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'mean_length': mean_length\n",
    "    })\n",
    "\n",
    "# === Save summary table ===\n",
    "results_df = pd.DataFrame(results)\n",
    "results_path = \"/Users/maartendoekhie/Desktop/lunar_lander/results/sensitivity_results.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# === Optional: rank by mean reward ===\n",
    "sorted_df = results_df.sort_values(by='mean_reward', ascending=False)\n",
    "print(\"\\nTop configurations by mean reward:\")\n",
    "print(sorted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_gpu_env)",
   "language": "python",
   "name": "ml_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
